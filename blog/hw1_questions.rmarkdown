---
title: "A Replication of Karlan and List (2007)"
author: "Andrew Wang"
date: today
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---




## Introduction

Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the _American Economic Review_ in 2007.Standard Letter was a regular appeal without mention of any matching funds.
Matching Grant was a letter stating that contributions would be matched dollar-for-dollar (or at different match ratios such as 2:1 or 3:1) by a lead donor.
Challenge Grant was a letter stating that the lead donor would only contribute if enough donations were received from others.The goal was to test both behavioral economic predictions (such as social pressure or anchoring) and standard economic theory regarding how incentives affect giving behavior. The randomized design ensures that differences in donation behavior across groups can be causally attributed to the letter variation. The results provided key insights into fundraising strategy and the psychology of donors, with implications for both nonprofits and economic theory on altruism. The article and supporting data are available from the [AEA website](https://www.aeaweb.org/articles?id=10.1257/aer.97.5.1774) and from Innovations for Poverty Action as part of [Harvard's Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/27853&version=4.2).


## Data

### Description
The dataset comprises 50,083 observations and 51 variables from a large-scale field experiment conducted by Karlan and List (2007) to study charitable giving behavior. Each row represents an individual donor who received one of several versions of a fundraising letter. The variables include binary indicators for treatment assignment (treatment, control), experimental conditions such as match ratio (ratio, ratio2, ratio3) and match threshold (size, size25, size50, size100, sizeno), as well as customized donation suggestions (ask, askd1, askd2, askd3).

Donation behavior is captured through variables like amount (the donation amount), gave (whether a donation was made), and amountchange (change from prior donation levels). Historical donation behavior is also recorded, including past frequency (freq), recency (years, dormant), and donor history (hpa, mrm2, ltmedmra). Demographic and geographic context is richly detailed with fields such as female, couple, state50one, and nonlit, along with state-level political and socioeconomic indicators (e.g., perbush, red0, median_hhincome, powner, psch_atlstba, pop_propurban).

The dataset is well-structured for causal analysis, containing a mixture of binary, categorical, and continuous variables. While most fields are complete, a few demographic columns (e.g., female, couple, pwhite) have some missing values. Overall, this dataset provides a robust foundation for replicating and extending the original experimental findings.

### Load and Inspect the Data



```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse))
```

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(haven)
library(knitr)

# Load the data
df <- read_dta("karlan_list_2007.dta")

# Preview structure of the data
neat_glimpse <- function(df, n = 5) {
  library(dplyr)
  library(knitr)

  df %>%
    summarise(across(everything(), typeof)) %>%
    pivot_longer(cols = everything(), names_to = "Variable", values_to = "Type") %>%
    mutate(
      Example = sapply(df, function(x) paste0(head(x, n), collapse = ", "))
    ) %>%
    kable(caption = "Clean Summary of Dataset Variables", align = "lll")
}
neat_glimpse(df)

```



### Summary Statistics

Below is a summary of the numeric and categorical variables in the dataset:



```{r, message=FALSE, warning=FALSE}
# Summary stats as a table
summary_stats <- summary(df)
kable(summary_stats, caption = "Summary Statistics of All Variables")
```



### Missing and Unique Values

This table shows the number of missing values and unique values for each column:



```{r, message=FALSE, warning=FALSE}
missing_unique <- df %>%
  summarise(across(everything(), list(
    missing = ~sum(is.na(.)),
    unique = ~n_distinct(.)
  )))

kable(missing_unique, caption = "Missing Values and Unique Counts per Variable")
```



:::: {.callout-note collapse="true"}
### Variable Definitions

| Variable             | Description                                                         |
|----------------------|---------------------------------------------------------------------|
| `treatment`          | Treatment                                                           |
| `control`            | Control                                                             |
| `ratio`              | Match ratio                                                         |
| `ratio2`             | 2:1 match ratio                                                     |
| `ratio3`             | 3:1 match ratio                                                     |
| `size`               | Match threshold                                                     |
| `size25`             | \$25,000 match threshold                                            |
| `size50`             | \$50,000 match threshold                                            |
| `size100`            | \$100,000 match threshold                                           |
| `sizeno`             | Unstated match threshold                                            |
| `ask`                | Suggested donation amount                                           |
| `askd1`              | Suggested donation was highest previous contribution                |
| `askd2`              | Suggested donation was 1.25 x highest previous contribution         |
| `askd3`              | Suggested donation was 1.50 x highest previous contribution         |
| `ask1`               | Highest previous contribution (for suggestion)                      |
| `ask2`               | 1.25 x highest previous contribution (for suggestion)               |
| `ask3`               | 1.50 x highest previous contribution (for suggestion)               |
| `amount`             | Dollars given                                                       |
| `gave`               | Gave anything                                                       |
| `amountchange`       | Change in amount given                                              |
| `hpa`                | Highest previous contribution                                       |
| `ltmedmra`           | Small prior donor: last gift was less than median \$35              |
| `freq`               | Number of prior donations                                           |
| `years`              | Number of years since initial donation                              |
| `year5`              | At least 5 years since initial donation                             |
| `mrm2`               | Number of months since last donation                                |
| `dormant`            | Already donated in 2005                                             |
| `female`             | Female                                                              |
| `couple`             | Couple                                                              |
| `state50one`         | State tag: 1 for one observation of each of 50 states; 0 otherwise  |
| `nonlit`             | Nonlitigation                                                       |
| `cases`              | Court cases from state in 2004-5 in which organization was involved |
| `statecnt`           | Percent of sample from state                                        |
| `stateresponse`      | Proportion of sample from the state who gave                        |
| `stateresponset`     | Proportion of treated sample from the state who gave                |
| `stateresponsec`     | Proportion of control sample from the state who gave                |
| `stateresponsetminc` | stateresponset - stateresponsec                                     |
| `perbush`            | State vote share for Bush                                           |
| `close25`            | State vote share for Bush between 47.5% and 52.5%                   |
| `red0`               | Red state                                                           |
| `blue0`              | Blue state                                                          |
| `redcty`             | Red county                                                          |
| `bluecty`            | Blue county                                                         |
| `pwhite`             | Proportion white within zip code                                    |
| `pblack`             | Proportion black within zip code                                    |
| `page18_39`          | Proportion age 18-39 within zip code                                |
| `ave_hh_sz`          | Average household size within zip code                              |
| `median_hhincome`    | Median household income within zip code                             |
| `powner`             | Proportion house owner within zip code                              |
| `psch_atlstba`       | Proportion who finished college within zip code                     |
| `pop_propurban`      | Proportion of population urban within zip code                      |

::::


## Balance Test 

As an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.


### Balance Testing on Pre-Treatment Variables



```{r, message=FALSE, warning=FALSE}
# Load necessary libraries
suppressPackageStartupMessages({
  library(tidyverse)
  library(broom)
  library(knitr)
})

# Manual t-test function using class formula
manual_t_test <- function(var, group_var, group1 = 1, group2 = 0, data) {
  x1 <- data[[var]][data[[group_var]] == group1]
  x2 <- data[[var]][data[[group_var]] == group2]

  x1 <- x1[!is.na(x1)]
  x2 <- x2[!is.na(x2)]

  n1 <- length(x1)
  n2 <- length(x2)
  m1 <- mean(x1)
  m2 <- mean(x2)
  s1 <- var(x1)
  s2 <- var(x2)

  se_diff <- sqrt(s1 / n1 + s2 / n2)
  t_stat <- (m1 - m2) / se_diff

  tibble(
    variable = var,
    group1_mean = m1,
    group2_mean = m2,
    mean_diff = m1 - m2,
    se = se_diff,
    t_stat = t_stat
  )
}

# Variables to test
test_vars <- c("mrm2", "years", "freq")

# Run manual t-tests
t_test_results <- map_dfr(test_vars, ~manual_t_test(.x, "treatment", data = df))

# Run regressions and extract treatment coefficient
regression_results <- map_dfr(test_vars, function(var) {
  mod <- lm(as.formula(paste(var, "~ treatment")), data = df)
  tidy(mod) %>%
    filter(term == "treatment") %>%
    mutate(variable = var) %>%
    select(variable, estimate, std.error, statistic)
})

# Merge results
results_combined <- left_join(
  t_test_results,
  regression_results,
  by = "variable"
) %>%
  rename(
    reg_estimate = estimate,
    reg_se = std.error,
    reg_t_stat = statistic
  )

```




### Experimental Results


```{r, echo=FALSE, message=FALSE, warning=FALSE}
# Display results
kable(results_combined, caption = "Balance Test: Manual T-Tests vs Linear Regression",
      digits = 3)
```



The balance test results shown above compare key baseline characteristics across the treatment and control groups. Specifically, I tested three variables—mrm2 (months since last donation), years (years since initial donation), and freq (number of prior donations)—to assess whether the treatment and control groups are statistically significantly different from one another.

In all three cases, the difference in means between the two groups is extremely small, and the associated t-statistics are all very close to zero, well below the critical value for 95% significance. These results are confirmed by linear regressions of each variable on the treatment indicator, which yield the exact same estimates and test statistics as the manual t-tests—demonstrating analytical consistency.

These findings suggest that the random assignment worked as intended, and there are no systematic differences between treatment and control groups on these pre-treatment variables. This supports the internal validity of the experimental design.

#### Conclusion: Corresponds to Results in Table 1
Table 1 in the original Karlan and List (2007) paper serves a critical diagnostic purpose. It provides summary statistics for a wide range of baseline variables, broken down by treatment and control groups, including demographic, behavioral, and geographic characteristics. Its role is to demonstrate balance across groups, validating the assumption that any differences in outcomes can be causally attributed to the treatment rather than pre-existing differences.

By comparing your balance test results to Table 1:

You can confirm that your replication dataset matches the original in both structure and balance.

Your t-test and regression results on mrm2, years, and freq closely align with the reported means in Table 1.

Like in Table 1, you find no statistically significant differences—reinforcing that the randomization mechanism was successful.

In short, Table 1 provides the foundational evidence that the experimental groups are statistically equivalent at baseline. Your balance test serves the same function, and by mirroring that analysis, you’re showing your replication is on solid footing.


##### Reference: Karlan and List (2007)

Here is the PDF of the original study and includes Table 1, which summarizes the balance between treatment and control groups:

You can also [download the PDF here](karlan_list_2007.pdf).


### Charitable Contribution Made

First, I analyze whether matched donations lead to an increased response rate of making a donation. 

### Donation Rate by Treatment Status



```{r, message=FALSE, warning=FALSE}
library(tidyverse)

# Calculate donation proportions
donation_rate <- df %>%
  group_by(treatment) %>%
  summarise(prop_donated = mean(gave, na.rm = TRUE)) %>%
  mutate(group = if_else(treatment == 1, "Treatment", "Control"))

# Ensure control is first
donation_rate$group <- factor(donation_rate$group, levels = c("Control", "Treatment"))

# Plot with zoomed y-axis (proportions, not percent)
ggplot(donation_rate, aes(x = group, y = prop_donated)) +
  geom_col(fill = "steelblue") +
  labs(
    title = "Proportion of Donors by Treatment Group",
    x = "Group",
    y = "Proportion Donated"
  ) +
  scale_y_continuous(limits = c(0, 0.03)) +
  theme_minimal(base_size = 14)
```





### Effect of Treatment on Donation Behavior



```{r, message=FALSE, warning=FALSE}
# Load required libraries
suppressPackageStartupMessages({
  library(tidyverse)
  library(broom)
  library(knitr)
})

# Manual t-test using class formula
treat <- df$gave[df$treatment == 1]
ctrl <- df$gave[df$treatment == 0]

# Remove missing
treat <- na.omit(treat)
ctrl <- na.omit(ctrl)

# Compute values
mean_treat <- mean(treat)
mean_ctrl <- mean(ctrl)
n1 <- length(treat)
n0 <- length(ctrl)
var1 <- var(treat)
var0 <- var(ctrl)

se_diff <- sqrt(var1 / n1 + var0 / n0)
t_stat <- (mean_treat - mean_ctrl) / se_diff

# Assemble t-test result
t_test_result <- tibble(
  `Mean (Treatment)` = mean_treat,
  `Mean (Control)` = mean_ctrl,
  `Difference` = mean_treat - mean_ctrl,
  `Std. Error` = se_diff,
  `T-Statistic` = t_stat
)

# Run regression
reg_model <- lm(gave ~ treatment, data = df)
reg_summary <- tidy(reg_model) %>%
  filter(term == "treatment") %>%
  select(`Coefficient` = estimate,
         `Std. Error` = std.error,
         `T-Statistic` = statistic,
         `P-Value` = p.value)

# Output formatted tables
kable(t_test_result, caption = "Manual T-Test: Effect of Treatment on Donations", digits = 4)
kable(reg_summary, caption = "Linear Regression: Coefficient on Treatment", digits = 4)
```


The analysis shows that individuals in the treatment group—those who received a message mentioning matched donations—were more likely to donate than those in the control group. The difference in donation rates is small in absolute terms (about 0.4 percentage points), but statistically significant. Both the manual t-test and the linear regression confirm this result, with t-statistics above 3 and a p-value well below the 5% significance threshold.

In practical terms, this suggests that even a subtle change in messaging—like telling donors their gift will be matched—can meaningfully affect behavior. People appear to be motivated by the idea that their donation will go further, or that someone else values their contribution enough to match it. This reflects broader behavioral principles such as reciprocity, social influence, and the perceived value of impact.

Overall, the findings support the conclusion that matching offers are an effective strategy in charitable fundraising, nudging more people to take action even when the individual gain is not large. This result is consistent with what is reported in Table 2a, Panel A of the original Karlan and List (2007) paper.


### Probit Regression: Effect of Treatment on Donation



```{r, message=FALSE, warning=FALSE}
# Load required libraries
suppressPackageStartupMessages({
  library(tidyverse)
  library(broom)
})

# Run probit regression
probit_model <- glm(gave ~ treatment, family = binomial(link = "probit"), data = df)

# Tidy output
probit_summary <- tidy(probit_model)

# Show results in table
library(knitr)
kable(probit_summary, caption = "Probit Regression: Outcome = Gave, Predictor = Treatment", digits = 4)
```



The probit regression estimates the effect of being assigned to the treatment group on the probability of making a donation. The coefficient on the `treatment` variable is positive and statistically significant, closely matching the result reported in Table 3, Column 1 of Karlan and List (2007).

In practical terms, this confirms that simply offering a matching grant—without changing anything else about the appeal—significantly increases the likelihood that someone donates. While the actual donation rates remain low, this small but consistent shift in probability reflects how subtle cues in messaging can have real behavioral impact. The probit model accounts for the nonlinear relationship between predictors and a binary outcome, and still finds that the treatment has a meaningful effect.

This reinforces the key insight of the paper: **people are more likely to act generously when they believe their contribution will be leveraged or matched**, even in a large, field-based real-world setting.



### Differences between Match Rates

Next, I assess the effectiveness of different sizes of matched donations on the response rate.

### Effect of Match Size on Donation (T-Tests)



```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)

# Filter treatment group and relabel ratio values
df_treat <- df %>%
  filter(treatment == 1) %>%
  mutate(ratio = as.character(ratio)) %>%
  mutate(ratio_label = case_when(
    ratio == "1" ~ "1:1",
    ratio == "2" ~ "2:1",
    ratio == "3" ~ "3:1",
    TRUE ~ NA_character_
  ))

# Safe t-test function
compare_match_rates <- function(group1, group2, data) {
  x1 <- na.omit(data$gave[data$ratio_label == group1])
  x2 <- na.omit(data$gave[data$ratio_label == group2])

  if (length(x1) < 5 | length(x2) < 5) {
    return(tibble(
      comparison = paste(group1, "vs", group2),
      estimate1 = NA, estimate2 = NA, statistic = NA,
      p.value = NA, conf.low = NA, conf.high = NA
    ))
  }

  t.test(x1, x2, var.equal = TRUE) %>%
    tidy() %>%
    mutate(comparison = paste(group1, "vs", group2)) %>%
    select(comparison, estimate1, estimate2,
           statistic, p.value, conf.low, conf.high)
}

# Run all 3 comparisons
results <- bind_rows(
  compare_match_rates("1:1", "2:1", df_treat),
  compare_match_rates("2:1", "3:1", df_treat),
  compare_match_rates("1:1", "3:1", df_treat)
)

# Output results
kable(results, caption = "Pairwise T-Tests: Donation Rates by Match Size", digits = 4)
```


On page 8, Karlan and List (2007) write:

“...the figures suggest that larger match ratios are not necessarily more effective.”

The results support this observation: even though the match ratios increase from 1:1 to 3:1, the donation rates barely change, and the differences are not statistically significant. The marginal value of increasing the match ratio appears to be negligible.

This suggests that it's the presence of a match offer that matters—not the size of the match. Once donors know their gift will be matched, making it a 2:1 or 3:1 offer doesn't significantly change their likelihood to give.


### Regression: Effect of Match Ratio Size on Donation



```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)

# Create dummy variables for match ratios (treatment only)
df <- df %>%
  mutate(
    ratio = as.character(ratio),
    ratio1 = as.integer(ratio == "1"),
    ratio2 = as.integer(ratio == "2"),
    ratio3 = as.integer(ratio == "3")
  )

# Run regression using ratio1, ratio2, ratio3 (baseline is control group)
match_lm <- lm(gave ~ ratio1 + ratio2 + ratio3, data = df)

# Tidy and print results
match_lm_tidy <- tidy(match_lm)

kable(match_lm_tidy, caption = "Regression: Effect of Match Ratio on Donation (Baseline = Control)", digits = 4)
```



#### Key Findings:
Control group baseline: 1.79% donation rate (Intercept)

1:1 match: Increases donation rate by ~0.29 percentage points (not statistically significant, p = 0.097)

2:1 match: Increases donation rate by ~0.48 points (statistically significant, p = 0.0061)

3:1 match: Similar increase as 2:1 (~0.49 points), also statistically significant (p = 0.0051)

The presence of any match offer increases the probability of donating, but interestingly, larger match ratios (2:1 and 3:1) are more effective than 1:1. This differs slightly from the earlier t-tests (which found no significant differences between match sizes), but the regression shows that the increase in donation probability for 2:1 and 3:1 offers is statistically significant compared to control.

That said, the difference between 2:1 and 3:1 is very small, and the coefficients are nearly identical, reinforcing the earlier point: increasing the match size beyond 2:1 might not yield additional behavioral gains.

The regression results show that larger match ratios (2:1 and 3:1) significantly increase the likelihood of donating relative to no match, while the 1:1 match is only marginally effective. However, the impact of 2:1 and 3:1 matches is nearly identical, suggesting that while higher match ratios can enhance giving, the effect appears to plateau beyond 2:1. This nuance helps explain why the authors state that “larger match ratios are not necessarily more effective” (p. 8)—the increase from 1:1 to 2:1 matters, but going from 2:1 to 3:1 doesn't do much more.

### Differences in Match Size Effects: Raw vs Fitted



```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)

# Filter treatment group and clean up ratio variable
df_treat <- df %>%
  filter(treatment == 1) %>%
  mutate(ratio = as.character(ratio))

# Calculate raw response rates by ratio
raw_rates <- df_treat %>%
  filter(ratio %in% c("1", "2", "3")) %>%
  group_by(ratio) %>%
  summarise(response_rate = mean(gave, na.rm = TRUE)) %>%
  pivot_wider(names_from = ratio, values_from = response_rate) %>%
  rename(`1:1` = `1`, `2:1` = `2`, `3:1` = `3`) %>%
  mutate(
    raw_diff_2v1 = `2:1` - `1:1`,
    raw_diff_3v2 = `3:1` - `2:1`
  )

# Pull regression estimates
reg_diffs <- match_lm_tidy %>%
  filter(term %in% c("ratio1", "ratio2", "ratio3")) %>%
  select(term, estimate) %>%
  pivot_wider(names_from = term, values_from = estimate) %>%
  mutate(
    fitted_diff_2v1 = ratio2 - ratio1,
    fitted_diff_3v2 = ratio3 - ratio2
  )

# Combine raw + regression diff output
comparison <- bind_cols(
  raw_rates %>% select(raw_diff_2v1, raw_diff_3v2),
  reg_diffs %>% select(fitted_diff_2v1, fitted_diff_3v2)
)

kable(comparison, caption = "Difference in Response Rates: Raw vs Fitted Coefficients", digits = 4)
```


Both the raw data and the regression coefficients show that increasing the match from 1:1 to 2:1 leads to a small but noticeable increase in donation rates. However, increasing the match further to 3:1 provides no meaningful gain. This supports the authors’ suggestion that higher match ratios do not necessarily produce better outcomes. In essence, the presence of a match offer matters—but making that match larger beyond a certain point (2:1) does not meaningfully change donor behavior. This has important practical implications: organizations may not need to offer higher matches to motivate giving, as the psychological effect appears to plateau.


### Size of Charitable Contribution

In this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.

### Effect of Treatment on Donation Amount (Full Sample)



```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)

# T-test: donation amount by treatment
tt <- t.test(amount ~ treatment, data = df)

# Format t-test result
tt_df <- tidy(tt) %>%
  mutate(
    estimate1 = tt$estimate[1],
    estimate2 = tt$estimate[2]
  ) %>%
  select(estimate1, estimate2, estimate, statistic, p.value, conf.low, conf.high)

# Regression: amount ~ treatment
lm_amt <- lm(amount ~ treatment, data = df)
lm_df <- tidy(lm_amt) %>%
  filter(term == "treatment")

# Output both tables
kable(tt_df, caption = "T-Test: Effect of Treatment on Donation Amount", digits = 4)
kable(lm_df, caption = "Regression: Treatment Effect on Donation Amount", digits = 4)
```


The difference is about $0.15 higher in the treatment group.

The t-test p-value is 0.0551, just slightly above the 5% threshold.

The regression gives nearly identical results: estimate = 0.1536, p = 0.0628

#### Results Summary
The treatment group gave more on average, but the difference is not statistically significant at the 5% level—it’s just above the line. While this might suggest that match offers increase total donation amounts, the evidence is not strong enough to confidently claim a real effect at conventional significance levels.

### Treatment Effect on Donation Amount (Among Donors Only)



```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(broom)
library(knitr)

# Filter to people who donated something
df_donors <- df %>% filter(gave == 1)

# T-test
tt_donors <- t.test(amount ~ treatment, data = df_donors)

# Format t-test output
tt_donors_df <- tidy(tt_donors) %>%
  mutate(
    estimate1 = tt_donors$estimate[1],
    estimate2 = tt_donors$estimate[2]
  ) %>%
  select(estimate1, estimate2, estimate, statistic, p.value, conf.low, conf.high)

# Regression
lm_donors <- lm(amount ~ treatment, data = df_donors)
lm_donors_df <- tidy(lm_donors) %>%
  filter(term == "treatment")

# Output
kable(tt_donors_df, caption = "T-Test: Treatment Effect on Donation Amount (Donors Only)", digits = 4)
kable(lm_donors_df, caption = "Regression: Treatment Effect on Donation Amount (Donors Only)", digits = 4)
```


The t-test p-value = 0.559

The regression p-value = 0.5615

Both show no statistically significant difference between treatment and control among donors.

#### Conclusion
Among people who chose to donate, those in the treatment group did not give more—in fact, they gave slightly less on average, though not by a statistically meaningful amount.

Limiting the analysis to only those who donated, we find no statistically significant difference in average donation amount between the treatment and control groups. The treatment coefficient is not significant, and the observed difference (~$1.67) is small and negative. However, this regression does not have a causal interpretation because it conditions on giving—an outcome influenced by the treatment. Thus, while match offers may increase the number of donors, they do not appear to affect how much people give once they’ve already decided to donate

### Distribution of Donation Amounts (Among Donors Only)



```{r, message=FALSE, warning=FALSE}
library(tidyverse)

# Filter donors only
donors_only <- df %>% filter(gave == 1)

# Compute group means
group_means <- donors_only %>%
  group_by(treatment) %>%
  summarise(avg = mean(amount, na.rm = TRUE))

# Label treatment vs control
donors_only <- donors_only %>%
  mutate(group = if_else(treatment == 1, "Treatment", "Control"))

# Plot
ggplot(donors_only, aes(x = amount)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "white") +
  geom_vline(data = group_means %>%
               mutate(group = if_else(treatment == 1, "Treatment", "Control")),
             aes(xintercept = avg), color = "red", linetype = "dashed", linewidth = 1) +
  facet_wrap(~ group, scales = "free_y") +
  coord_cartesian(xlim = c(0, 200)) +
  labs(
    title = "Distribution of Donation Amounts (Among Donors)",
    x = "Donation Amount ($)",
    y = "Count"
  ) +
  theme_minimal(base_size = 14)
```




## Simulation Experiment

As a reminder of how the t-statistic "works," in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.

Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. 

Further suppose that the true distribution of respondents who do get a charitable donation match of any size  is Bernoulli with probability p=0.022 that a donation is made.

### Law of Large Numbers
#### Simulated Sampling Distribution of ATE (Average Treatment Effect)



```{r, message=FALSE, warning=FALSE}
library(tidyverse)

# Pull full donation amount vectors
treat <- df %>% filter(treatment == 1) %>% pull(amount)
control <- df %>% filter(treatment == 0) %>% pull(amount)

# Set sample size per experiment
n <- 100  # sample size per group per experiment

# Simulate 10,000 experiments
set.seed(42)
n_sim <- 10000
diffs <- replicate(n_sim, {
  t_sample <- sample(treat, n, replace = TRUE)
  c_sample <- sample(control, n, replace = TRUE)
  mean(t_sample) - mean(c_sample)
})

# Compute cumulative average of differences
cum_avg <- cumsum(diffs) / seq_along(diffs)
true_diff <- mean(treat) - mean(control)

# Plot it
tibble(iteration = 1:n_sim, cum_avg = cum_avg) %>%
  ggplot(aes(x = iteration, y = cum_avg)) +
  geom_line(color = "steelblue", linewidth = 1) +
  geom_hline(yintercept = true_diff, color = "red", linetype = "dashed", linewidth = 1) +
  labs(
    title = "Convergence of Estimated ATE Over Simulated Experiments",
    x = "Simulation Number",
    y = "Cumulative Average Treatment Effect"
  ) +
  theme_minimal(base_size = 14)
```



This plot visualizes the convergence of estimated treatment effects over 10,000 simulated randomized experiments. In each simulation, we draw a fresh random sample of 100 units from both the treatment and control groups and compute the difference in average donation amounts. The blue line shows the cumulative average of these estimates across simulations, and the dashed red line represents the true difference in the full dataset. As expected, the cumulative average stabilizes around the true value as the number of simulations increases. This demonstrates the Law of Large Numbers in action and shows how sampling variability decreases with repeated estimation, reinforcing the reliability of our causal estimate under random assignment.


### Central Limit Theorem



```{python, message=FALSE, warning=FALSE}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset (make sure the path works in your environment)
df = pd.read_stata("karlan_list_2007.dta")

# Prepare data
treat = df[df["treatment"] == 1]["amount"].dropna().values
control = df[df["treatment"] == 0]["amount"].dropna().values

sample_sizes = [50, 200, 500, 1000]
true_diff = np.mean(treat) - np.mean(control)

fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes = axes.flatten()

np.random.seed(123)
for i, n in enumerate(sample_sizes):
    diffs = [np.mean(np.random.choice(treat, n, replace=True)) -
             np.mean(np.random.choice(control, n, replace=True)) for _ in range(1000)]
    
    ax = axes[i]
    sns.histplot(diffs, bins=30, kde=True, color="skyblue", ax=ax)
    ax.axvline(0, color="red", linestyle="--", linewidth=1.2, label="Zero")
    ax.axvline(true_diff, color="green", linestyle="--", linewidth=1.2, label="True Diff")
    ax.set_title(f"Sample size = {n}")
    ax.set_xlabel("Estimated Treatment Effect")
    ax.set_ylabel("Count")

axes[0].legend()
fig.suptitle("Central Limit Theorem: Distribution of Mean Differences", fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()
```



#### Observations:
At n = 50: The distribution is wide and noisy, and zero is close to the center — we often estimate a treatment effect near zero just by chance. The green line is close, but it’s hard to distinguish from noise.

At n = 200: The distribution narrows slightly, and the green line (true effect) starts pulling away from zero. However, there's still a decent chance of drawing a sample where zero falls near the center of the distribution.

At n = 500 and 1000: The distribution becomes tight and centered around the true effect. The red line (zero) is now clearly in the tails, suggesting that it's very unlikely for a large random sample to produce an estimate near zero if a true treatment effect exists.

#### Conclusion:
As sample size increases, the sampling distribution of the estimated treatment effect becomes more concentrated around the true value, and the probability of observing a misleading result (like a difference close to zero when there is a real effect) diminishes. This demonstrates the precision and reliability gained from larger samples, as predicted by the Central Limit Theorem.









